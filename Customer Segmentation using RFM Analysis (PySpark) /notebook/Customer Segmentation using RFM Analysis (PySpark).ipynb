{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545e07de-6397-434a-b779-f64faebafb74",
   "metadata": {},
   "source": [
    "üõçÔ∏è RFM Analysis for Customer Segmentation\n",
    "Objective:\n",
    "Identify and categorize customers based on Recency, Frequency, and Monetary metrics to improve marketing strategies and retention.\n",
    "\n",
    "Dataset:\n",
    "\n",
    "Retail transactions dataset\n",
    "\n",
    "Columns include: InvoiceDate, Customer ID, Revenue, etc.\n",
    "\n",
    "Cleaned & preprocessed before analysis\n",
    "\n",
    "Approach:\n",
    "\n",
    "Data cleaning and preparation\n",
    "\n",
    "Calculate Recency, Frequency, and Monetary values\n",
    "\n",
    "Assign RFM scores using quintiles (1‚Äì5)\n",
    "\n",
    "Combine scores to create RFM segments\n",
    "\n",
    "Visualize customer distribution by segment\n",
    "\n",
    "Provide actionable insights"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8dbd2577-ebdc-4b7d-a77c-714c0426e68c",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£ Spark Session Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd23fd3d-a912-416a-8a5d-964403c8796f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/11 12:19:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set PySpark environment variables to use Anaconda Python (customize these paths as needed)\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a new Spark session for this retail project\n",
    "spark=SparkSession.builder.appName('Retail_Project1').getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc0304f9-7611-437a-a50f-acdea89b0b7b",
   "metadata": {},
   "source": [
    "2Ô∏è‚É£ Data Conversion: Excel ‚û°Ô∏è CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bf660-0ab3-49c2-9ef2-27f561e9d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the raw retail data from an Excel file (source format)\n",
    "df_excel=pd.read_excel(\"online_retail_II.xlsx\")\n",
    "\n",
    "# Convert the Excel sheet to CSV so Spark can easily consume it\n",
    "df_excel.to_csv(\"retail_project_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7e2231a-7a47-450d-8848-5e07c9426704",
   "metadata": {},
   "source": [
    "3Ô∏è‚É£ Load Data in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ffdf8-320c-410a-97dc-043cccf51310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load CSV data into Spark DataFrame\n",
    "df=spark.read.csv(\"retail_project_data.csv\",header=True)\n",
    "\n",
    "# Show first 5 rows and infer schema\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "637a838c-921e-4908-99e5-337d1dce710f",
   "metadata": {},
   "source": [
    "Check the initial schema - note most columns are loaded as strings, which needs correction.\n",
    "\n",
    "4Ô∏è‚É£ Redefine Schema for Proper Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11f883-e167-4a25-9101-f540ed7e75b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The default schema treats many fields as strings: we need to specify actual data types\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Invoice\", StringType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", TimestampType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "    StructField(\"Customer ID\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])\n",
    "\n",
    "df=spark.read.csv(\"retail_project_data.csv\",header=True,schema=schema)      \n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c45efeee-5e3a-4353-bad2-fba34273640a",
   "metadata": {},
   "source": [
    "5Ô∏è‚É£ Data Overview and Null Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50dacc-7ff0-40ff-9c6c-7dccc0718aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize dataset: count, mean, stddev, min, max for each numeric column\n",
    "df.describe().collect()\n",
    "\n",
    "# Check number of nulls in each column\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "df.select([_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c485596-6455-4981-99a2-16989fbcc141",
   "metadata": {},
   "source": [
    "6Ô∏è‚É£ Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d86ef-f86b-40b0-9322-422055e74b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove problematic rows according to business rules:\n",
    "# 1. Remove transactions with missing Customer ID.\n",
    "# 2. Remove cancellation invoices (Invoice starting with 'C').\n",
    "# 3. Remove rows with negative/zero Quantity or Price.\n",
    "\n",
    "df=df.filter(col('Customer ID').isNotNull())\n",
    "df=df.filter(~col('Invoice').startswith('C'))          # Tilde (~): NOT operator in PySpark\n",
    "df=df.filter(df.Quantity>0).filter(df.Price>0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6856e0-22dd-4c1a-b5fb-3f1a9e72a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This way:\n",
    "\n",
    "Null Customer ID ‚Üí dropped\n",
    "\n",
    "Cancellation invoices ‚Üí dropped\n",
    "\n",
    "Negative/zero quantities ‚Üí dropped\n",
    "\n",
    "Negative/zero prices ‚Üí dropped\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fb3df-a2c6-456d-9e4a-4f09fcfd609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can filter in both ways:\n",
    "'''\n",
    "\n",
    "df.filter(col...)\n",
    "or\n",
    "df.filter(df.col_name....)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98006d9b-8017-4a14-8d9e-8a45bfddf476",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122492f-de1e-45f2-a8fc-bd7be4be95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10k nearly rows are dropped"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c21d31f-b26d-4931-b69f-7cee74d8a9e4",
   "metadata": {},
   "source": [
    "7Ô∏è‚É£ Feature Engineering: Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01719ff8-8f8b-48c6-80a4-77a489726b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Revenue for each row (i.e., revenue = quantity * price)\n",
    "\n",
    "df=df.withColumn('Revenue',df.Price * df.Quantity)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "467fd070-7c76-4fad-90dc-b8e17bc12161",
   "metadata": {},
   "source": [
    "8Ô∏è‚É£ Aggregation Examples\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b56caca-bd95-4b4c-93f5-c8b3480bc682",
   "metadata": {},
   "source": [
    "Revenue by Country and Invoice Date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ab911-0e34-4b64-9325-f427d33b1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate: total revenue per Country and InvoiceDate\n",
    "\n",
    "df.groupBy('Country','InvoiceDate').sum('Revenue').withColumnRenamed('sum_Rev','total_Rev').show()  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bde80c9-daff-46ab-ad78-07b8744af419",
   "metadata": {},
   "source": [
    "Revenue by Year, Month, and Country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7b4b5-d768-49f4-89ba-e12e2a3ff93c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, month, year\n",
    "\n",
    "# Extract year and month for time-based analysis\n",
    "df=df.withColumn('InvoiceDate',to_date(col('InvoiceDate'), 'yyyy-MM-dd'))\n",
    "df=df.withColumn('month',month(col('InvoiceDate')))\n",
    "df=df.withColumn('year',year(col('InvoiceDate')))\n",
    "\n",
    "\n",
    "# Aggregate revenue over year, month, and country\n",
    "df.groupBy('year','month','Country').sum('Revenue').withColumnRenamed('sum_Rev','total_Rev').orderBy('year','month','Country').show() "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d7f790f-c7fa-4e65-b9fc-d630032288a8",
   "metadata": {},
   "source": [
    "9Ô∏è‚É£ Save Cleaned Results for Downstream Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87af29-112b-43b4-ab7b-7d9e3be7dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned DataFrame and aggregated DataFrames as Parquet for efficient storage/reloading\n",
    "df.write.mode('overwrite').parquet('cleaned_DataFrame')\n",
    "\n",
    "agg_df_RbyCC=df.groupBy('Country','InvoiceDate').sum('Revenue').withColumnRenamed('sum_Rev','total_Rev')\n",
    "\n",
    "agg_df_RbyMC=df.groupBy('year','month','Country').sum('Revenue').withColumnRenamed('sum_Rev','total_Rev').orderBy('year','month','Country')\n",
    "\n",
    "agg_df_RbyCC.write.mode('overwrite').parquet('RbyCC')\n",
    "agg_df_RbyMC.write.mode('overwrite').parquet('RbyMC')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b63ce69-1b42-433b-a88c-445758e92f2f",
   "metadata": {},
   "source": [
    "Reload parquet with: spark.read.parquet(\"filename\")\n",
    "\n",
    "üîü Top-N Customers by Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5b8ef-d642-4e81-aef6-7d338f54d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10 customers by total revenue\n",
    "\n",
    "df_cleaned=spark.read.parquet('cleaned_DataFrame')\n",
    "top_N_cust=df_cleaned.groupBy('Customer ID').sum('Revenue').orderBy(col('sum(Revenue)').desc()).limit(10).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d72cd-bef6-4958-8c5f-517fa8bc10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthly_trend=df_cleaned.groupBy(\"year\", \"month\").sum(\"Revenue\").orderBy(col(\"year\").desc(), col(\"month\").desc()).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8efd5d-9c41-4915-a8d6-df13d99a4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01111879-f3c7-42f6-bd63-15a362d2af2e",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£1Ô∏è‚É£ RFM Analysis: Recency, Frequency, Monetary\n",
    "\n",
    "What is RFM?\n",
    "\n",
    "Recency: How recently did the customer buy? (days since last purchase)\n",
    "\n",
    "Frequency: How often do they buy? (number of purchases)\n",
    "\n",
    "Monetary: How much do they spend? (total revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc556c-8c0f-44af-ab26-80ba990426d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM: Key metrics for customer segmentation\n",
    "\n",
    "from pyspark.sql.functions import max, current_date, datediff,lit,countDistinct\n",
    "\n",
    "# Find the most recent InvoiceDate in the dataset to define \"today\"\n",
    "max_date = df_cleaned.select(max(\"InvoiceDate\")).collect()[0][0]\n",
    "\n",
    "# Recency: Days since last purchase for each customer\n",
    "recency_df = df_cleaned.groupBy(\"Customer ID\").agg(datediff(lit(max_date), max(\"InvoiceDate\")).alias(\"Recency\")).orderBy(col('Recency').desc())\n",
    "\n",
    "# Frequency: Count of unique invoices per customer\n",
    "frequency_df = df_cleaned.groupBy(\"Customer ID\").agg(countDistinct(\"Invoice\").alias(\"Frequency\")).orderBy(col('Frequency'))\n",
    "\n",
    "# Monetary: Total revenue per customer\n",
    "monetary_df=df_cleaned.groupBy('Customer ID').sum('Revenue').withColumnRenamed('sum(Revenue)','Monetary').orderBy(col('Monetary').desc())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24a1e043-0399-417c-8b7b-3ede0926f9cc",
   "metadata": {},
   "source": [
    "Combine into RFM DataFrame:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b907e-d681-4ee4-b5f9-342e5c59ed53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join R, F, M metrics into a single DataFrame\n",
    "RFM_df=recency_df.join(frequency_df, 'Customer ID').join(monetary_df,'Customer ID')\n",
    "RFM_df.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbb668db-2970-4664-8636-5240d512c2af",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£2Ô∏è‚É£ Assign RFM Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d06fe4-fe8d-4d5e-bc58-769762c82459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile,concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Score: assign 1-5 bucket to each metric (1: best, 5: worst for recency; 5: best for frequency/monetary)\n",
    "\n",
    "\n",
    "recency_score=RFM_df.withColumn('recency_score', ntile(5).over(Window.orderBy(col(\"Recency\").asc())))\n",
    "frequency_score=RFM_df.withColumn('frequency_score', ntile(5).over(Window.orderBy(col(\"Frequency\").desc())))\n",
    "monetary_score=RFM_df.withColumn('monetary_score', ntile(5).over(Window.orderBy(col(\"Monetary\").desc())))\n",
    "\n",
    "# Combine all scores for each customer\n",
    "RFM_score=recency_score.join(frequency_score, on='Customer ID').join(monetary_score, on='Customer ID')\n",
    "\n",
    "\n",
    "# Concatenated and total RFM score\n",
    "RFM_score_combibed=RFM_score.withColumn('RFM_score_combined',(concat_ws(\"\",col('recency_score'),col('frequency_score'),col('monetary_score'))))\n",
    "RFM_score_total=RFM_score.withColumn('RFM_score_total',col('recency_score')+col('frequency_score')+col('monetary_score'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41c73fa6-df4a-43c1-9e7b-09e96b7e364a",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£3Ô∏è‚É£ Segment Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dcf3f2-5e05-46a7-9a6c-0169c18034f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segement analysis::\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Assign each customer to a segment based on their total RFM score\n",
    "RFM_segmented = RFM_score_total.withColumn(\n",
    "    \"Segment\",\n",
    "    when(col(\"RFM_score_total\") >= 13, \"Champions\") \\\n",
    "    .when((col(\"RFM_score_total\") >= 10) & (col(\"RFM_score_total\") <= 12), \"Loyal Customers\") \\\n",
    "    .when((col(\"RFM_score_total\") >= 7) & (col(\"RFM_score_total\") <= 9), \"Potential Loyalist\") \\\n",
    "    .when((col(\"RFM_score_total\") >= 4) & (col(\"RFM_score_total\") <= 6), \"Needs Attention\") \\\n",
    "    .otherwise(\"At Risk\")\n",
    ")\n",
    "RFM_segmented.show(20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "966ac654-6da6-4305-88a5-f7dd2a78ae94",
   "metadata": {},
   "source": [
    "1Ô∏è‚É£4Ô∏è‚É£ RFM Segment Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fb5dd-783e-4676-869f-c1890370291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_segmented.groupBy(\"Segment\").count().orderBy(col(\"count\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ceeb55-55b1-42e0-a6ab-06caed3637bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for plotting\n",
    "segment_counts = RFM_segmented.groupBy(\"Segment\").count().toPandas()\n",
    "# Sort by count descending\n",
    "segment_counts = segment_counts.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(segment_counts[\"Segment\"], segment_counts[\"count\"], color=\"skyblue\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Customer Segment\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.title(\"RFM Customer Segmentation Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62cc6e-908b-45d8-badf-062af1ad6fb0",
   "metadata": {},
   "source": [
    "üìå Conclusion\n",
    "In this project, we performed an RFM (Recency, Frequency, Monetary) analysis on transactional data using PySpark. The workflow involved:\n",
    "\n",
    "ETL & Data Cleaning: Removed null values, filtered out negative quantities/prices, and calculated Revenue.\n",
    "\n",
    "RFM Calculation: Computed Recency, Frequency, and Monetary metrics for each customer.\n",
    "\n",
    "Scoring: Assigned RFM scores using ntile() for segmentation ranking.\n",
    "\n",
    "Segmentation: Categorized customers into meaningful groups such as Champions, Loyal Customers, At Risk, etc.\n",
    "\n",
    "Visualization: Created bar charts to show distribution of customer segments.\n",
    "\n",
    "Key Insights\n",
    "Champions and Loyal Customers represent the largest segments, indicating a strong base of repeat buyers.\n",
    "\n",
    "At Risk customers should be targeted with retention campaigns before they churn.\n",
    "\n",
    "Potential Loyalists could be converted into long-term buyers with personalized offers.\n",
    "\n",
    "The distribution of segments shows opportunities for both retention and growth marketing.\n",
    "\n",
    "Business Implications\n",
    "Retention Strategies: Special offers, loyalty programs, and personalized communication for At Risk and Needs Attention segments.\n",
    "\n",
    "Upselling/Cross-selling: Focus on Champions and Loyal Customers with premium products or bundles.\n",
    "\n",
    "Nurturing New Buyers: Engage Potential Loyalists to increase purchase frequency.\n",
    "\n",
    "Limitations & Next Steps\n",
    "The dataset size is relatively small; results may differ with larger, more diverse data.\n",
    "\n",
    "Segmentation was based only on RFM; additional behavioral and demographic data could improve targeting.\n",
    "\n",
    "Future work could integrate time series trend analysis, predictive modeling (e.g., churn prediction), or recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
