{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545e07de-6397-434a-b779-f64faebafb74",
   "metadata": {},
   "source": [
    "ðŸ›ï¸ RFM Analysis for Customer Segmentation\n",
    "Objective:\n",
    "Identify and categorize customers based on Recency, Frequency, and Monetary metrics to improve marketing strategies and retention.\n",
    "\n",
    "Dataset:\n",
    "\n",
    "Retail transactions dataset\n",
    "\n",
    "Columns include: InvoiceDate, Customer ID, Revenue, etc.\n",
    "\n",
    "Cleaned & preprocessed before analysis\n",
    "\n",
    "Approach:\n",
    "\n",
    "Data cleaning and preparation\n",
    "\n",
    "Calculate Recency, Frequency, and Monetary values\n",
    "\n",
    "Assign RFM scores using quintiles (1â€“5)\n",
    "\n",
    "Combine scores to create RFM segments\n",
    "\n",
    "Visualize customer distribution by segment\n",
    "\n",
    "Provide actionable insights"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8dbd2577-ebdc-4b7d-a77c-714c0426e68c",
   "metadata": {},
   "source": [
    "1ï¸âƒ£ Spark Session Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd23fd3d-a912-416a-8a5d-964403c8796f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/11 12:19:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set PySpark environment variables to use Anaconda Python (customize these paths as needed)\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start a new Spark session for this retail project\n",
    "spark=SparkSession.builder.appName('Retail_Project1').getOrCreate()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc0304f9-7611-437a-a50f-acdea89b0b7b",
   "metadata": {},
   "source": [
    "2ï¸âƒ£ Data Conversion: Excel âž¡ï¸ CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133bf660-0ab3-49c2-9ef2-27f561e9d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the raw retail data from an Excel file (source format)\n",
    "df_excel=pd.read_excel(\"online_retail_II.xlsx\")\n",
    "\n",
    "# Convert the Excel sheet to CSV so Spark can easily consume it\n",
    "df_excel.to_csv(\"retail_project_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7e2231a-7a47-450d-8848-5e07c9426704",
   "metadata": {},
   "source": [
    "3ï¸âƒ£ Load Data in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ffdf8-320c-410a-97dc-043cccf51310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load CSV data into Spark DataFrame\n",
    "df=spark.read.csv(\"retail_project_data.csv\",header=True)\n",
    "\n",
    "# Show first 5 rows and infer schema\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "637a838c-921e-4908-99e5-337d1dce710f",
   "metadata": {},
   "source": [
    "Check the initial schema - note most columns are loaded as strings, which needs correction.\n",
    "\n",
    "4ï¸âƒ£ Redefine Schema for Proper Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab11f883-e167-4a25-9101-f540ed7e75b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The default schema treats many fields as strings: we need to specify actual data types\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Invoice\", StringType(), True),\n",
    "    StructField(\"StockCode\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"InvoiceDate\", TimestampType(), True),\n",
    "    StructField(\"Price\", DoubleType(), True),\n",
    "    StructField(\"Customer ID\", StringType(), True),\n",
    "    StructField(\"Country\", StringType(), True)\n",
    "])\n",
    "\n",
    "df=spark.read.csv(\"retail_project_data.csv\",header=True,schema=schema)      \n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c45efeee-5e3a-4353-bad2-fba34273640a",
   "metadata": {},
   "source": [
    "5ï¸âƒ£ Data Overview and Null Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50dacc-7ff0-40ff-9c6c-7dccc0718aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize dataset: count, mean, stddev, min, max for each numeric column\n",
    "df.describe().collect()\n",
    "\n",
    "# Check number of nulls in each column\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "df.select([_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c485596-6455-4981-99a2-16989fbcc141",
   "metadata": {},
   "source": [
    "6ï¸âƒ£ Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d86ef-f86b-40b0-9322-422055e74b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove problematic rows according to business rules:\n",
    "# 1. Remove transactions with missing Customer ID.\n",
    "# 2. Remove cancellation invoices (Invoice starting with 'C').\n",
    "# 3. Remove rows with negative/zero Quantity or Price.\n",
    "\n",
    "df=df.filter(col('Customer ID').isNotNull())\n",
    "df=df.filter(~col('Invoice').startswith('C'))          # Tilde (~): NOT operator in PySpark\n",
    "df=df.filter(df.Quantity>0).filter(df.Price>0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6856e0-22dd-4c1a-b5fb-3f1a9e72a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This way:\n",
    "\n",
    "Null Customer ID â†’ dropped\n",
    "\n",
    "Cancellation invoices â†’ dropped\n",
    "\n",
    "Negative/zero quantities â†’ dropped\n",
    "\n",
    "Negative/zero prices â†’ dropped\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846fb3df-a2c6-456d-9e4a-4f09fcfd609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can filter in both ways:\n",
    "'''\n",
    "\n",
    "df.filter(col...)\n",
    "or\n",
    "df.filter(df.col_name....)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98006d9b-8017-4a14-8d9e-8a45bfddf476",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c122492f-de1e-45f2-a8fc-bd7be4be95ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10k nearly rows are dropped"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c21d31f-b26d-4931-b69f-7cee74d8a9e4",
   "metadata": {},
   "source": [
    "7ï¸âƒ£ Feature Engineering: Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01719ff8-8f8b-48c6-80a4-77a489726b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Revenue for each row (i.e., revenue = quantity * price)\n",
    "\n",
    "df=df.withColumn('Revenue',df.Price * df.Quantity)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "467fd070-7c76-4fad-90dc-b8e17bc12161",
   "metadata": {},
   "source": [
    "8ï¸âƒ£ Aggregation Examples\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b56caca-bd95-4b4c-93f5-c8b3480bc682",
   "metadata": {},
   "source": [
    "Revenue by Country and Invoice Date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ab911-0e34-4b64-9325-f427d33b1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate: total revenue per Country and InvoiceDate\n",
    "\n",
    "df.groupBy('Country','InvoiceDate').sum('Revenue').withColumnRenamed('sum_Rev','total_Rev').show()  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9bde80c9-daff-46ab-ad78-07b8744af419",
   "metadata": {},
   "source": [
    "Revenue by Year, Month, and Country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7b4b5-d768-49f4-89ba-e12e2a3ff93c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, month, year\n",
    "\n",
    "# Extract year and month for time-based analysis\n",
    "df=df.withColumn('InvoiceDate',to_date(col('InvoiceDate'), 'yyyy-MM-dd'))\n",
    "df=df.withColumn('month',month(col('InvoiceDate')))\n",
    "df=df.withColumn('year',year(col('InvoiceDate')))\n",
    "\n",
    "\n",
    "# Aggregate revenue over year, month, and country\n",
    "df.groupBy('year','month','Country').sum('Revenue').withColumnRenamed('sum_Rev','total_Rev').orderBy('year','month','Country').show() "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d7f790f-c7fa-4e65-b9fc-d630032288a8",
   "metadata": {},
   "source": [
    "9ï¸âƒ£ Save Cleaned Results for Downstream Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d87af29-112b-43b4-ab7b-7d9e3be7dd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned DataFrame and aggregated DataFrames as Parquet for efficient storage/reloading\n",
    "df.write.mode('overwrite').parquet('cleaned_DataFrame')\n",
    "\n",
    "agg_df_RbyCC=df.groupBy('Country','InvoiceDate').sum('Revenue').withColumnRenamed('sum_Rev','total_Rev')\n",
    "\n",
    "agg_df_RbyMC=df.groupBy('year','month','Country').sum('Revenue').withColumnRenamed('sum_Rev','total_Rev').orderBy('year','month','Country')\n",
    "\n",
    "agg_df_RbyCC.write.mode('overwrite').parquet('RbyCC')\n",
    "agg_df_RbyMC.write.mode('overwrite').parquet('RbyMC')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b63ce69-1b42-433b-a88c-445758e92f2f",
   "metadata": {},
   "source": [
    "Reload parquet with: spark.read.parquet(\"filename\")\n",
    "\n",
    "ðŸ”Ÿ Top-N Customers by Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba5b8ef-d642-4e81-aef6-7d338f54d852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10 customers by total revenue\n",
    "\n",
    "df_cleaned=spark.read.parquet('cleaned_DataFrame')\n",
    "top_N_cust=df_cleaned.groupBy('Customer ID').sum('Revenue').orderBy(col('sum(Revenue)').desc()).limit(10).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816d72cd-bef6-4958-8c5f-517fa8bc10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#monthly_trend=df_cleaned.groupBy(\"year\", \"month\").sum(\"Revenue\").orderBy(col(\"year\").desc(), col(\"month\").desc()).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8efd5d-9c41-4915-a8d6-df13d99a4961",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "01111879-f3c7-42f6-bd63-15a362d2af2e",
   "metadata": {},
   "source": [
    "1ï¸âƒ£1ï¸âƒ£ RFM Analysis: Recency, Frequency, Monetary\n",
    "\n",
    "What is RFM?\n",
    "\n",
    "Recency: How recently did the customer buy? (days since last purchase)\n",
    "\n",
    "Frequency: How often do they buy? (number of purchases)\n",
    "\n",
    "Monetary: How much do they spend? (total revenue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc556c-8c0f-44af-ab26-80ba990426d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM: Key metrics for customer segmentation\n",
    "\n",
    "from pyspark.sql.functions import max, current_date, datediff,lit,countDistinct\n",
    "\n",
    "# Find the most recent InvoiceDate in the dataset to define \"today\"\n",
    "max_date = df_cleaned.select(max(\"InvoiceDate\")).collect()[0][0]\n",
    "\n",
    "# Recency: Days since last purchase for each customer\n",
    "recency_df = df_cleaned.groupBy(\"Customer ID\").agg(datediff(lit(max_date), max(\"InvoiceDate\")).alias(\"Recency\")).orderBy(col('Recency').desc())\n",
    "\n",
    "# Frequency: Count of unique invoices per customer\n",
    "frequency_df = df_cleaned.groupBy(\"Customer ID\").agg(countDistinct(\"Invoice\").alias(\"Frequency\")).orderBy(col('Frequency'))\n",
    "\n",
    "# Monetary: Total revenue per customer\n",
    "monetary_df=df_cleaned.groupBy('Customer ID').sum('Revenue').withColumnRenamed('sum(Revenue)','Monetary').orderBy(col('Monetary').desc())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "24a1e043-0399-417c-8b7b-3ede0926f9cc",
   "metadata": {},
   "source": [
    "Combine into RFM DataFrame:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b907e-d681-4ee4-b5f9-342e5c59ed53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Join R, F, M metrics into a single DataFrame\n",
    "RFM_df=recency_df.join(frequency_df, 'Customer ID').join(monetary_df,'Customer ID')\n",
    "RFM_df.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbb668db-2970-4664-8636-5240d512c2af",
   "metadata": {},
   "source": [
    "1ï¸âƒ£2ï¸âƒ£ Assign RFM Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d06fe4-fe8d-4d5e-bc58-769762c82459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import ntile,concat_ws\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Score: assign 1-5 bucket to each metric (1: best, 5: worst for recency; 5: best for frequency/monetary)\n",
    "\n",
    "\n",
    "recency_score=RFM_df.withColumn('recency_score', ntile(5).over(Window.orderBy(col(\"Recency\").asc())))\n",
    "frequency_score=RFM_df.withColumn('frequency_score', ntile(5).over(Window.orderBy(col(\"Frequency\").desc())))\n",
    "monetary_score=RFM_df.withColumn('monetary_score', ntile(5).over(Window.orderBy(col(\"Monetary\").desc())))\n",
    "\n",
    "# Combine all scores for each customer\n",
    "RFM_score=recency_score.join(frequency_score, on='Customer ID').join(monetary_score, on='Customer ID')\n",
    "\n",
    "\n",
    "# Concatenated and total RFM score\n",
    "RFM_score_combibed=RFM_score.withColumn('RFM_score_combined',(concat_ws(\"\",col('recency_score'),col('frequency_score'),col('monetary_score'))))\n",
    "RFM_score_total=RFM_score.withColumn('RFM_score_total',col('recency_score')+col('frequency_score')+col('monetary_score'))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41c73fa6-df4a-43c1-9e7b-09e96b7e364a",
   "metadata": {},
   "source": [
    "1ï¸âƒ£3ï¸âƒ£ Segment Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dcf3f2-5e05-46a7-9a6c-0169c18034f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segement analysis::\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Assign each customer to a segment based on their total RFM score\n",
    "RFM_segmented = RFM_score_total.withColumn(\n",
    "    \"Segment\",\n",
    "    when(col(\"RFM_score_total\") >= 13, \"Champions\") \\\n",
    "    .when((col(\"RFM_score_total\") >= 10) & (col(\"RFM_score_total\") <= 12), \"Loyal Customers\") \\\n",
    "    .when((col(\"RFM_score_total\") >= 7) & (col(\"RFM_score_total\") <= 9), \"Potential Loyalist\") \\\n",
    "    .when((col(\"RFM_score_total\") >= 4) & (col(\"RFM_score_total\") <= 6), \"Needs Attention\") \\\n",
    "    .otherwise(\"At Risk\")\n",
    ")\n",
    "RFM_segmented.show(20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "966ac654-6da6-4305-88a5-f7dd2a78ae94",
   "metadata": {},
   "source": [
    "1ï¸âƒ£4ï¸âƒ£ RFM Segment Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fb5dd-783e-4676-869f-c1890370291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RFM_segmented.groupBy(\"Segment\").count().orderBy(col(\"count\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ceeb55-55b1-42e0-a6ab-06caed3637bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for plotting\n",
    "segment_counts = RFM_segmented.groupBy(\"Segment\").count().toPandas()\n",
    "# Sort by count descending\n",
    "segment_counts = segment_counts.sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(segment_counts[\"Segment\"], segment_counts[\"count\"], color=\"skyblue\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Customer Segment\")\n",
    "plt.ylabel(\"Number of Customers\")\n",
    "plt.title(\"RFM Customer Segmentation Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e62cc6e-908b-45d8-badf-062af1ad6fb0",
   "metadata": {},
   "source": [
    "ðŸ“Œ Conclusion\n",
    "In this project, we performed an RFM (Recency, Frequency, Monetary) analysis on transactional data using PySpark. The workflow involved:\n",
    "\n",
    "ETL & Data Cleaning: Removed null values, filtered out negative quantities/prices, and calculated Revenue.\n",
    "\n",
    "RFM Calculation: Computed Recency, Frequency, and Monetary metrics for each customer.\n",
    "\n",
    "Scoring: Assigned RFM scores using ntile() for segmentation ranking.\n",
    "\n",
    "Segmentation: Categorized customers into meaningful groups such as Champions, Loyal Customers, At Risk, etc.\n",
    "\n",
    "Visualization: Created bar charts to show distribution of customer segments.\n",
    "\n",
    "Key Insights\n",
    "Champions and Loyal Customers represent the largest segments, indicating a strong base of repeat buyers.\n",
    "\n",
    "At Risk customers should be targeted with retention campaigns before they churn.\n",
    "\n",
    "Potential Loyalists could be converted into long-term buyers with personalized offers.\n",
    "\n",
    "The distribution of segments shows opportunities for both retention and growth marketing.\n",
    "\n",
    "Business Implications\n",
    "Retention Strategies: Special offers, loyalty programs, and personalized communication for At Risk and Needs Attention segments.\n",
    "\n",
    "Upselling/Cross-selling: Focus on Champions and Loyal Customers with premium products or bundles.\n",
    "\n",
    "Nurturing New Buyers: Engage Potential Loyalists to increase purchase frequency.\n",
    "\n",
    "Limitations & Next Steps\n",
    "The dataset size is relatively small; results may differ with larger, more diverse data.\n",
    "\n",
    "Segmentation was based only on RFM; additional behavioral and demographic data could improve targeting.\n",
    "\n",
    "Future work could integrate time series trend analysis, predictive modeling (e.g., churn prediction), or recommendation systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
